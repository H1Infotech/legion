\section{Implementation of Resilience}

\subsection{\texttt{need\_preserve} Implementation}

tagging region instances
tagging tasks

-the mapper marks an instance as persistent, i.e., vector<vector<bool>> persistent
-in finalize\_map\_task, the singleTask sees this and notes it down in the Individual\_task's persistent\_tasks list
-when trigger\_complete is called on the task, 

	1) inside line 5560, invalidate\_region\_tree\_contexts, 
		inside which we have runtime->forest->invalidate\_versions, we do not do on region[idx].region.
		we also do not do the instance\_top\_views
		
	2) we retain the mark on the task as allowed\_for\_gc.

   3) we go to the incoming of this task, and just like verified\_regions[true], we mark outgoing\_edge\_dominated[true]
      if all the outgoing of a task is marked as true, then we change allowed\_for\_gc = true.


Discussion Points for today
--------------------------
1) allow persistence on a subset of mapped instances in map\_task ? Pro:
flexibility, Con: if a checkpoint is to be considered useful for a restart, not
having the full set of inputs checkpointed seems contradictory. 
2) verify\_regions tracks op dependencies after physical dependence analysis, correct ? 
3) a discussion on persistence inside map\_task call 


discussed design:

1) build a function set\_hardened\_instnace(instance,task) along the lines of the set\_gc\_priority(instance, never, task) inside the mapper call
2) inside that mark a task's incoming edges as saying that it leads to a hardened instance(task)
3) the garbage collector will basically collect a task whose outgoing edges are all marked as verified/hardened.
4) if a task is gc'ed, then it marks all its incoming edges as leads to a hardened instance.
5) steps 1-4 will be based on set\_garbage\_collection\_priority and how verified\_regions are set. We will be adding a new list to each task, similar to verified\_regions, that will represent edges\_ending\_in\_hardened\_regions.


quash\_operation:
  847 line in realm/proc\_impl.cc tells us that we should set the poisoned to true and ensure that the start\_event.has\_triggered\_faultaware(poisoned) returns true

what about two sibling tasks that map the same instance, and one hardens it, the other does not
    - alex raised this, we discussed that trigger recover would check 
    - but the way we are doing it now, we are doing unverfied\_regions.erase based on even on guy below us who will call harden on a region instance
    - so the parent would have gone away, but the child B who did not call harden ont he physical region should be able to restart since the instance will be there
how to show thsi is working, if the application is too fast, then delete meta task on the logical region is called which deletes the region irrespective of whether its GC\_NEVER\_PRIORITY

REACTIVE
faulty task
   catch it
   poison propagation
   identify recovery point
   trigger\_recover

SEMANTICS
regions and instances and consistency

PROACTIVE
    mark instances as harden
    
DIFFERENCE WITH PARSEC:
    label a task with snapshot, i.e., all or none policy
    whereas we allow selective instances to be marked as harden.
      - what is an example app that is benefited by this ?

\subsubsection{Interaction of need\_preserve with the commit wavefront}
\subsubsection{Obtaining dependence graph in the mapper before calling need\_preserve} 


\subsection{ProfilingResponse callback used}
while using the profiling measurement reporting infrastructure as-is causes overhead, this is because of the invoking of a mapper profiling response function. We do not need that for resilience, so the resilience callback would short circuit it.

task launch -> porfiling reported event is there -> when it is triggered -> \texttt{singletask::profiling\_response} is invoked -> handle things
   ----- in here, there is no need to call the mapper side yet. So, avoid one overhead there, maybe this was never called and so , this is not an optimization. ok, back to square one. 

 
